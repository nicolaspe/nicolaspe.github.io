<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title> Heart of a Survivor - npe </title>
        <link rel="stylesheet" href="../../src/npe_cont.css">
        <script src="../../src/js/header_vis.js"></script>
        <link href="../src/favicon.ico" rel="icon">
    </head>

    <body>
        <div id="sketch"></div>

        <header>
            <span class="title"><a href="/">NE</a></span>
            <nav>
                <a href="/">work</a>
                
                <a href="/about">about</a>
            </nav>
        </header>

        <main> 
<h1>Heart of a Survivor</h1>
<p>Heart of a Survivor (working title) is the prototype for an unreleased festival-bound virtual reality experience, that takes the audience through a story of loss, love and hope: a journey through the heart of a genocide survivor.</p>
<p><code>As this is a yet-unreleased piece, I will not disclose specific details about the piece, instead focusing on the creative and technical journey.</code></p>
<figure class="proj_img proj_img_full" style="text-align:center">
	<img class="p_capture" src="./media/stayalive.png" alt="Heart Survivor, prototype scene">
</figure>
<h2 id="adapting-screenplay">Adapting screenplay</h2>
<p>I worked alongside <a href="https://www.ejectnow.com/">Gabriel Brasil</a>, fellow technologist and art director, to take the script of this heart-wrenching story and transform it to an immersive VR piece. Being a new medium, VR doesn’t have an established language as cinema or theatre. As such, it falls into the shoulders of us technologist to work alonside the writers and directors to adapt their scripts, showing them what’s possible in this emerging environment and employing the full extent of the opportunities it provides.</p>
<div class="line-group">
<mark class="sideR">The core questions when starting a VR experience: "Why is this in VR? How does it use its capabilities? Who is the user and what can the user do?"</mark>
<div>
  <figure class="vid_container vid_16x9" style="text-align:center">
    <iframe src="https://player.vimeo.com/video/358901555" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
  </figure>
  <figcaption>Ideation session timelapse - edited by Gabriel Brasil</figcaption>
</div>
</div>
<p>The script was thoroughly dissected and analyzed from several points of view: story beats and narrative structure, user placement and interaction possiblilities, technical elements and how they play along in VR, scene composition, and more. All these elements were then submitted to the director, presenting her with options and feedback, and starting a collaborative iteration in order to accomplish her view.</p>
<br>
<div class="line-group">
<figure class="proj_img proj_img_sideR" style="text-align:center">
    <img class="p_detail" src="./media/hs_heart00.jpg" alt="Concept drawing of the heart environment housing one of the memories">
</figure>
<p>The memories are recounted inside the metaphorical heart of the main character. The user embodies his presence in the memories, making them a key narrative tool. And as it’s not trying to replicate reality, the art direction reflected this from the beginning.</p>
</div>
<figure class="proj_img proj_img_full" style="text-align:center;display:block">
	<img class="p_detail" style="width:49.8%" src="./media/hs_heart02.jpg" alt="3d heart environment with template objects">
  <img class="p_detail" style="width:49.8%" src="./media/hs_heart03.jpg" alt="Wide shot of the 3d heart environment">
  <figcaption>3d heart environment that houses the entire experience</figcaption>
</figure>
<h2 id="designing-interactions">Designing interactions</h2>
<p>As an inherently immersive medium, the realm of possibilities of user interactions in VR has to be aligned with the idea of embodiment. As such, they have to be carefully designed. The interactions have to be unobtrussive and transparent, as a mechanic cannot take the user out of the experience.</p>
<p>This can be achieved in two ways. First, we can take advantage of movements that come natural to us, such as grabbing or reaching out to things. Or second, we can incorporate one new action in the beginning of the experience, reinforcing it over the first minutes, so that it becomes natural for the audience and they don’t have to actively think to perform it.</p>
<figure class="vid_container vid_720_full" style="text-align:center">
    <video class="vid_doc" controls>
        <source src="./media/demo_interaction.mp4" type="video/mp4">
    </video>
</figure>
<figcaption>Reaching out for a memory object activates the scene</figcaption>
<h2 id="animatics-%26-360-shooting">Animatics &amp; 360 shooting</h2>
<p>After designing the interactions, laying out the storyboards and with the script in a good state, we needed to prototype the current state to move forward. We recorded ourselves as scratch track and animated the 2d boards, and later a basic VR 3d blocking. Even with stock assets and minimal animations, we were able to get a feel of the pace, interactions and story inside the headset.</p>
<figure class="vid_container vid_720" style="text-align:center">
  <iframe src="https://player.vimeo.com/video/358558909" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
</figure>
<figcaption>View of 2d, 3d and volumetric animatics done with Gabriel Brasil</figcaption>

<br>
<div class="line-group">
<p>With the need to better understand and lock the motion script, we were joined by <a href="https://www.chrissyelie.com/">Chris Hall</a> to do a 360 shooting with professional actors. We played out the entire script, keeping track of motion and distance to the camera - an Insta 360, which let us quickly review the material on an Oculus Go.</p>
<figure class="proj_img proj_img_sideR" style="text-align:center">
    <img class="p_detail" src="./media/hs_360.jpg" alt="360 shooting day - photo by Gabriel Brasil">
</figure>
</div>
<h2 id="exploring-pipelines-for-volumetric-technologies">Exploring pipelines for volumetric technologies</h2>
<p>Early on in the process, it became evident that such a powerful and intimate story needed specialized elements to create a deep human connection. To replicate the subtleties of human microgestures, something standard motion capture cannot convey, we looked at volumetric capture technologies.</p>

<br>
<div class="line-group">
<div class="vid_side" style="text-align:center">
  <figure style="margin:0">
    <video class="vid_doc" controls muted>
        
        <source src="./media/hs_volumetric.mp4" type="video/mp4">
    
</figure>
<figcaption style="margin-bottom:0">Test of a volumetric asset by 4DSViews with custom shaders materials to achieve a dreamy feel</figcaption>
</div>
<p>First, we tested two different products from full 3d volumetric capture studios: <a href="https://www.4dviews.com/">4DSViews’ Holosys</a> and the <a href="https://www.microsoft.com/en-us/mixed-reality/capture-studios">Microsoft’s Mixed Reality Capture Studio (MRCS)</a>. Despite achieving decent results in a short time, the degree of manipulation, reliability and costs of these solutions meant that we could not work with them.</p>
</div>

<h3 id="depthkit-in-unrealeninge">DepthKit in UnrealEninge</h3>
<p>Then, we moved to <a href="https://www.depthkit.tv/">DepthKit</a>, a fantastic 2.5d solution for volumetric capture. As there’s no plugin for Unreal Engine, the first step was come up with a proper pipeline. The exported OBJ sequence was imported into Maya <a href="https://www.highend3d.com/maya/script/obj-i-o-obj-sequences-import-export-for-maya">with a special plugin</a>, and re-exported as an <em>alembic cache</em>. This proved to be effective, but the textures kept desynchronizing with the geometry cache. To solve it, I used <em>ffmpeg</em> to batch downscale the images and used the level sequencer to manipulate their playback.</p>

<div class="line-group">
<p>After getting the technology to work, I experimented with mixed alternatives. As a body’s performance can be replicated thanks to motion capture and DepthKit can capture the subtelties of facial performances, I combined both to leverage their strengths and cover their weaknesses: a virtual face transplant.</p>
<div class="vid_side" style="text-align:center">
  <figure style="margin:0">
    <video class="vid_doc" autoplay loop muted>
        
        <source src="./media/DK_face.mp4" type="video/mp4">
    
</figure>
  <figcaption>Exploring possibilities: transplanting a volumetric facial performance into a 3d body</figcaption>
</div>
</div>
<figure class="proj_img proj_img_full" style="text-align:center">
  <img class="p_detail" style="width:98%" src="./media/dk_studio01.jpg" alt="Team at Sensorium's DepthKit studio @ RLab">
  <br>
  <img class="p_detail" style="width:49%" src="./media/dk_studio02.jpg" alt="DepthKit recording at Sensorium's studio @ RLab"> 
	<img class="p_detail" style="width:49%" src="./media/dk_screen.jpg" alt="DepthKit recording screen">
  <figcaption>DepthKit recording at Sensorium's studio @ RLab</figcaption>
</figure>
<h2 id="creating-prototypes">Creating prototypes</h2>
<p>Finally, after getting the different pieces to work together, it was time to build a prototype of the entire experience. Each scene was created in it’s own level, as was the heart that acted as a hub between them. All of them were loaded on a persistent level, with a level sequencer controlling the visibility and playback of each individual scene.</p>
<br>
<figure class="vid_container vid_sq" style="text-align:center">
    <video class="vid_doc" controls>
        <source src="./media/hs_demo01.mp4" type="video/mp4">
        <source src="./media/hs_demo01.webm" type="video/webm">
    </video>
</figure>
<figure class="vid_container vid_sq" style="text-align:center">
    <video class="vid_doc" controls>
        <source src="./media/hs_demo02.mp4" type="video/mp4">
        <source src="./media/hs_demo02.webm" type="video/webm">
    </video>
</figure>
<figcaption>Prototypes of two scenes, incorporating interaction and volumetric assets</figcaption>
<figure class="proj_img proj_img_full" style="text-align:center">
  <img class="p_detail" src="./media/hs_jungle.jpg" alt="Prototype jungle scene"> 
<figcaption>Prototype of the jungle scene, combining 3d and point cloud assets</figcaption>
</figure>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>I would like to thank to all the amazing people I worked with and helped develop this project. Going through this process has granted me a deeper insight into the affordances of VR and the different tools and pipelines that are yet to be created for it.</p>
<ul>
<li>Victoria Bousis, director and writer</li>
<li><a href="https://www.ejectnow.com/">Gabriel Brasil</a></li>
<li><a href="https://toddjbryant.com/">Todd Bryant</a> and the <a href="https://www.rlab.nyc/">RLab</a></li>
<li>Matthew Niederhauser and John Fitzgerald, <a href="https://www.sensorium.works/">Sensorium</a></li>
<li><a href="https://www.chrissyelie.com/">Chris Hall</a></li>
<li><a href="http://mishazv.com/bio/">Misha Zabranska</a></li>
</ul>
<figure class="proj_img proj_img_full" style="text-align:center">
  <img class="p_detail" style="width:49%" src="./media/teamwork01.jpg" alt="Working on the project"> 
	<img class="p_detail" style="width:49%" src="./media/teamwork02.jpg" alt="Storyboarding and concept design">
</figure>
<p><a href="#" onclick="return history.go(-1),!0">&lt; Go Back</a></p>
 </main>

        <footer>
            <br>
            <p>website coded + designed by nicolás escarpentier - Sep 2019</p>
        </footer>
    </body>

</html>